{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\newargmining\\.venv\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import WhisperFeatureExtractor, WhisperModel, TrainingArguments, Trainer\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from datasets import load_from_disk, concatenate_datasets\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "MAX_TEXT_LEN = 128\n",
    "BATCH_SIZE = 8\n",
    "NUM_CLASSES = 6\n",
    "LEARNING_RATE = 2e-5\n",
    "EPOCHS = 10\n",
    "SEED = 42\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "def load_and_preprocess_in_batches(df, batch_size=100, output_dir='processed_batches_whisper'):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    total_rows = len(df)\n",
    "    for start_idx in range(0, total_rows, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, total_rows)\n",
    "        batch_df = df.iloc[start_idx:end_idx]\n",
    "        data = []\n",
    "        for idx, row in batch_df.iterrows():\n",
    "            raw_path = row[\"audio\"]\n",
    "            path = str(raw_path[0]) if isinstance(raw_path, list) else str(raw_path)\n",
    "            label = int(row[\"label\"])\n",
    "            try:\n",
    "                waveform, sample_rate = torchaudio.load(path)\n",
    "                data.append({\n",
    "                    \"audio\": {\n",
    "                        \"array\": waveform.squeeze().numpy(),\n",
    "                        \"sampling_rate\": sample_rate\n",
    "                    },\n",
    "                    \"label\": label\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {path}: {e}\")\n",
    "        if data:\n",
    "            batch_dataset = Dataset.from_list(data)\n",
    "            batch_file = os.path.join(output_dir, f'batch_{start_idx}_{end_idx}.arrow')\n",
    "            batch_dataset.save_to_disk(batch_file)\n",
    "            print(f\"Saved batch {start_idx}-{end_idx} to {batch_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping audio data...: 100%|██████████| 3388/3388 [00:01<00:00, 2517.30it/s]\n",
      "Building AFC Context: 100%|██████████| 3388/3388 [00:00<00:00, 13826.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SplitInfo(train=<mamkit.data.datasets.UnimodalDataset object at 0x0000023CC9311FC0>, val=<mamkit.data.datasets.UnimodalDataset object at 0x0000023CC9313B80>, test=<mamkit.data.datasets.UnimodalDataset object at 0x0000023CC9312E90>)\n",
      "1228\n",
      "0\n",
      "[list([WindowsPath('D:/newargmining/testdataset/MMUSED-fallacy/audio_clips/10_1984/87.wav')])\n",
      " list([WindowsPath('D:/newargmining/testdataset/MMUSED-fallacy/audio_clips/10_1984/149.wav')])\n",
      " list([WindowsPath('D:/newargmining/testdataset/MMUSED-fallacy/audio_clips/10_1984/146.wav')])\n",
      " ...\n",
      " list([WindowsPath('D:/newargmining/testdataset/MMUSED-fallacy/audio_clips/46_2020/136.wav')])\n",
      " list([WindowsPath('D:/newargmining/testdataset/MMUSED-fallacy/audio_clips/46_2020/313.wav')])\n",
      " list([WindowsPath('D:/newargmining/testdataset/MMUSED-fallacy/audio_clips/46_2020/1090.wav')])]\n",
      "[0 0 1 ... 4 4 5]\n",
      "                                                  audio label\n",
      "0     [D:\\newargmining\\testdataset\\MMUSED-fallacy\\au...     0\n",
      "1     [D:\\newargmining\\testdataset\\MMUSED-fallacy\\au...     0\n",
      "2     [D:\\newargmining\\testdataset\\MMUSED-fallacy\\au...     1\n",
      "3     [D:\\newargmining\\testdataset\\MMUSED-fallacy\\au...     0\n",
      "4     [D:\\newargmining\\testdataset\\MMUSED-fallacy\\au...     0\n",
      "...                                                 ...   ...\n",
      "1223  [D:\\newargmining\\testdataset\\MMUSED-fallacy\\au...     0\n",
      "1224  [D:\\newargmining\\testdataset\\MMUSED-fallacy\\au...     0\n",
      "1225  [D:\\newargmining\\testdataset\\MMUSED-fallacy\\au...     4\n",
      "1226  [D:\\newargmining\\testdataset\\MMUSED-fallacy\\au...     4\n",
      "1227  [D:\\newargmining\\testdataset\\MMUSED-fallacy\\au...     5\n",
      "\n",
      "[1228 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "base_data_path = Path.cwd().joinpath('testdataset')\n",
    "    \n",
    "    # Load the dataset using MAMKit\n",
    "from mamkit.data.datasets import MMUSEDFallacy, InputMode\n",
    "    \n",
    "mm_used_fallacy_loader = MMUSEDFallacy(\n",
    "        task_name='afc',               \n",
    "        input_mode=InputMode.AUDIO_ONLY,\n",
    "        base_data_path=base_data_path\n",
    "    )\n",
    "    \n",
    "    # Get the splits\n",
    "# splits = list(mm_used_fallacy_loader.get_splits(method_key='mancini-et-al-2024'))\n",
    "splits= mm_used_fallacy_loader.get_splits('mm-argfallacy-2025') \n",
    "    # For demonstration, we'll use one split (you can iterate through all)\n",
    "split = splits[0]\n",
    "\n",
    "print(split)\n",
    "    \n",
    "    # Get train, validation, and test datasets\n",
    "train_data = split.train\n",
    "val_data = split.val\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(val_data))\n",
    "print(train_data.inputs)\n",
    "print(train_data.labels)\n",
    "    \n",
    "\n",
    "# Extract inputs and labels\n",
    "inputs = train_data.inputs\n",
    "labels = train_data.labels\n",
    "\n",
    "def dataset_to_df(dataset):\n",
    "    return pd.DataFrame({\n",
    "        \"audio\": dataset.inputs,\n",
    "        \"label\": dataset.labels,\n",
    "    })\n",
    "\n",
    "train_df = dataset_to_df(split.train)\n",
    "\n",
    "print(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6695781298e493d96c84e0561362de6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved batch 0-100 to processed_batches_whisper\\batch_0_100.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7092e31a5b26414295adf428eda24967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved batch 100-200 to processed_batches_whisper\\batch_100_200.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "950bb6b390db4470949ee198275faea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved batch 200-300 to processed_batches_whisper\\batch_200_300.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f1233167303401496d5b3c702ec434d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved batch 300-400 to processed_batches_whisper\\batch_300_400.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dcf690ebeb24825946a063f6b93928b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved batch 400-500 to processed_batches_whisper\\batch_400_500.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2807b337c33d4a8a81a867a9e9899de8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved batch 500-600 to processed_batches_whisper\\batch_500_600.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad7d78589dee4bdaac801f58ff181614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved batch 600-700 to processed_batches_whisper\\batch_600_700.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48aaacd3aacd4c8b9f1233f916844b59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved batch 700-800 to processed_batches_whisper\\batch_700_800.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd9ae56f56c94e0bb17170b3f5b27797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved batch 800-900 to processed_batches_whisper\\batch_800_900.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61b1cbdc02a54e1ca0288c13b417aa46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved batch 900-1000 to processed_batches_whisper\\batch_900_1000.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4863c657b7bf4a2db7a447115bdabf14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved batch 1000-1100 to processed_batches_whisper\\batch_1000_1100.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "338cb8bcc67647c6b1da0dbfb84376ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved batch 1100-1200 to processed_batches_whisper\\batch_1100_1200.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33bb2e6145cd4bae93e565a6b778658e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/28 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved batch 1200-1228 to processed_batches_whisper\\batch_1200_1228.arrow\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'df' is your DataFrame\n",
    "load_and_preprocess_in_batches(train_df, batch_size=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_all_batches(output_dir='processed_batches_whisper'):\n",
    "    batch_files = [os.path.join(output_dir, f) for f in os.listdir(output_dir) if f.endswith('.arrow')]\n",
    "    datasets = [load_from_disk(batch_file) for batch_file in batch_files]\n",
    "    full_dataset = concatenate_datasets(datasets)\n",
    "    return full_dataset\n",
    "\n",
    "# Load the full dataset\n",
    "full_dataset = load_all_batches()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['audio', 'label'],\n",
      "    num_rows: 1228\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(full_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Split into 80% train and 20% temp (which will be split into val and test)\n",
    "train_val_test = full_dataset.train_test_split(test_size=0.2, seed=SEED)\n",
    "\n",
    "# Step 2: Split the 20% temp into 50% validation and 50% test (i.e., 10% each of the original dataset)\n",
    "val_test = train_val_test['test'].train_test_split(test_size=0.5, seed=SEED)\n",
    "\n",
    "# Combine the splits into a DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_val_test['train'],\n",
    "    'validation': val_test['train'],\n",
    "    'test': val_test['test']\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoFeatureExtractor\n",
    "\n",
    "model_checkpoint = \"openai/whisper-small\"\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoFeatureExtractor\n",
    "\n",
    "# model_checkpoint = \"openai/whisper-small\"\n",
    "# feature_extractor = AutoFeatureExtractor.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fallacy_mapping = {\n",
    "    0: \"Appeal to Emotion\",\n",
    "    1: \"Appeal to Authority\",\n",
    "    2: \"Ad Hominem\",\n",
    "    3: \"False Cause\",\n",
    "    4: \"Slippery Slope\",\n",
    "    5: \"Slogans\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of WhisperForAudioClassification were not initialized from the model checkpoint at openai/whisper-small and are newly initialized: ['model.classifier.bias', 'model.classifier.weight', 'model.projector.bias', 'model.projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForAudioClassification, TrainingArguments, Trainer\n",
    "\n",
    "num_labels = 6\n",
    "model = AutoModelForAudioClassification.from_pretrained( model_checkpoint, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WhisperForAudioClassification(\n",
      "  (encoder): WhisperEncoder(\n",
      "    (conv1): Conv1d(80, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (conv2): Conv1d(768, 768, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "    (embed_positions): Embedding(1500, 768)\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x WhisperEncoderLayer(\n",
      "        (self_attn): WhisperSdpaAttention(\n",
      "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (activation_fn): GELUActivation()\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (projector): Linear(in_features=768, out_features=256, bias=True)\n",
      "  (classifier): Linear(in_features=256, out_features=6, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9ea293945614befa673f90fa8ebe97c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/982 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a0696ce17514a658871c5cebfe57177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/123 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4416a792bdca45e4b02b7f51d97828d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/123 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def extract_features(batch):\n",
    "    input_features = []\n",
    "    for audio in batch[\"audio\"]:\n",
    "        # Ensure audio is a dictionary with 'array' and 'sampling_rate'\n",
    "        if isinstance(audio, dict) and \"array\" in audio and \"sampling_rate\" in audio:\n",
    "            inputs = feature_extractor(\n",
    "                audio[\"array\"],\n",
    "                sampling_rate=audio[\"sampling_rate\"],\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            input_features.append(inputs.input_features[0])\n",
    "        else:\n",
    "            # Handle cases where audio is not in the expected format\n",
    "            input_features.append(None)\n",
    "    batch[\"input_features\"] = input_features\n",
    "    return batch\n",
    "\n",
    "\n",
    "dataset_dict = dataset_dict.map(\n",
    "    extract_features,\n",
    "    batched=True,\n",
    "    batch_size=4\n",
    ")\n",
    "\n",
    "# Set format for PyTorch\n",
    "columns = [\"input_features\", \"label\"]\n",
    "dataset_dict.set_format(type=\"torch\", columns=columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 8\n",
    "# training_args = TrainingArguments(output_dir=f\"{model_checkpoint}-finetuned-iemocap4\",\n",
    "#                                   evaluation_strategy=\"epoch\",\n",
    "#                                   learning_rate=3e-5,\n",
    "#                                   per_device_train_batch_size=batch_size,\n",
    "#                                   gradient_accumulation_steps=1, ### ??\n",
    "#                                   per_device_eval_batch_size=batch_size,\n",
    "#                                   num_train_epochs=5,\n",
    "#                                   warmup_ratio=0.1, ###?\n",
    "#                                   logging_steps=10,\n",
    "#                                   load_best_model_at_end=False,\n",
    "#                                   metric_for_best_model=\"accuracy\", ###?\n",
    "#                                   push_to_hub=True,\n",
    "#                                   hub_private_repo=True,\n",
    "#                                   report_to = 'wandb',\n",
    "#                                   run_name = 'Whisper-fine-tuning'                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class WhisperClassifier(nn.Module):\n",
    "#     def __init__(self, num_labels):\n",
    "#         super(WhisperClassifier, self).__init__()\n",
    "#         self.whisper = WhisperModel.from_pretrained(\"openai/whisper-small\")\n",
    "#         self.classifier = nn.Linear(self.whisper.config.hidden_size, num_labels)\n",
    "\n",
    "#     def forward(self, input_features, labels=None):\n",
    "#         # outputs = self.whisper(input_features=input_features)\n",
    "#         logits = self.classifier(outputs.last_hidden_state[:, 0, :])  # Using the first token's representation\n",
    "#         loss = None\n",
    "#         if labels is not None:\n",
    "#             loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "#         return {\"loss\": loss, \"logits\": logits}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds, average=\"macro\"),\n",
    "        \"precision\": precision_score(labels, preds, average=\"weighted\"),\n",
    "        \"recall\": recall_score(labels, preds, average=\"weighted\")\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\newargmining\\.venv\\lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\CPInS-01\\AppData\\Local\\Temp\\ipykernel_17780\\1161595529.py:18: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['audio', 'label', 'input_features'],\n",
      "    num_rows: 982\n",
      "})\n",
      "Dataset({\n",
      "    features: ['audio', 'label', 'input_features'],\n",
      "    num_rows: 123\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./whisper-fallacy\",\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "\n",
    "# model = WhisperClassifier(num_labels=6).to(device)\n",
    "\n",
    "print(dataset_dict['train'])\n",
    "print(dataset_dict['validation'])\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_dict['train'],\n",
    "    eval_dataset=dataset_dict['validation'],\n",
    "    tokenizer=feature_extractor,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1230' max='1230' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1230/1230 1:16:28, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.230639</td>\n",
       "      <td>0.593496</td>\n",
       "      <td>0.124150</td>\n",
       "      <td>0.352237</td>\n",
       "      <td>0.593496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.216116</td>\n",
       "      <td>0.593496</td>\n",
       "      <td>0.124150</td>\n",
       "      <td>0.352237</td>\n",
       "      <td>0.593496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.201973</td>\n",
       "      <td>0.593496</td>\n",
       "      <td>0.168317</td>\n",
       "      <td>0.441463</td>\n",
       "      <td>0.593496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.457390</td>\n",
       "      <td>0.569106</td>\n",
       "      <td>0.267785</td>\n",
       "      <td>0.526568</td>\n",
       "      <td>0.569106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.055300</td>\n",
       "      <td>1.706863</td>\n",
       "      <td>0.495935</td>\n",
       "      <td>0.230942</td>\n",
       "      <td>0.495070</td>\n",
       "      <td>0.495935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.055300</td>\n",
       "      <td>1.884981</td>\n",
       "      <td>0.569106</td>\n",
       "      <td>0.294848</td>\n",
       "      <td>0.554260</td>\n",
       "      <td>0.569106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.055300</td>\n",
       "      <td>2.024053</td>\n",
       "      <td>0.560976</td>\n",
       "      <td>0.263036</td>\n",
       "      <td>0.539090</td>\n",
       "      <td>0.560976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.055300</td>\n",
       "      <td>2.143887</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.249455</td>\n",
       "      <td>0.526524</td>\n",
       "      <td>0.544715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.232200</td>\n",
       "      <td>2.302490</td>\n",
       "      <td>0.536585</td>\n",
       "      <td>0.248392</td>\n",
       "      <td>0.527671</td>\n",
       "      <td>0.536585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.232200</td>\n",
       "      <td>2.318090</td>\n",
       "      <td>0.536585</td>\n",
       "      <td>0.245781</td>\n",
       "      <td>0.520376</td>\n",
       "      <td>0.536585</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\newargmining\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\newargmining\\.venv\\lib\\site-packages\\transformers\\configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n",
      "d:\\newargmining\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\newargmining\\.venv\\lib\\site-packages\\transformers\\configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n",
      "d:\\newargmining\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\newargmining\\.venv\\lib\\site-packages\\transformers\\configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n",
      "d:\\newargmining\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\newargmining\\.venv\\lib\\site-packages\\transformers\\configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n",
      "d:\\newargmining\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\newargmining\\.venv\\lib\\site-packages\\transformers\\configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n",
      "d:\\newargmining\\.venv\\lib\\site-packages\\transformers\\configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n",
      "d:\\newargmining\\.venv\\lib\\site-packages\\transformers\\configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n",
      "d:\\newargmining\\.venv\\lib\\site-packages\\transformers\\configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n",
      "d:\\newargmining\\.venv\\lib\\site-packages\\transformers\\configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n",
      "d:\\newargmining\\.venv\\lib\\site-packages\\transformers\\configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n",
      "d:\\newargmining\\.venv\\lib\\site-packages\\transformers\\configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1230, training_loss=0.5325281011379831, metrics={'train_runtime': 4590.2097, 'train_samples_per_second': 2.139, 'train_steps_per_second': 0.268, 'total_flos': 1.2330850849344e+18, 'train_loss': 0.5325281011379831, 'epoch': 10.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 00:21]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.318284273147583,\n",
       " 'eval_accuracy': 0.5365853658536586,\n",
       " 'eval_f1': 0.3168465824094963,\n",
       " 'eval_precision': 0.5032565871877206,\n",
       " 'eval_recall': 0.5365853658536586,\n",
       " 'eval_runtime': 23.5591,\n",
       " 'eval_samples_per_second': 5.221,\n",
       " 'eval_steps_per_second': 0.679,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(dataset_dict['test'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\newargmining\\.venv\\lib\\site-packages\\transformers\\configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./final-fallacy-classifier-whsipersmall\\\\preprocessor_config.json']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model, tokenizer and configuration\n",
    "model.save_pretrained(\"./final-fallacy-classifier-whispersmall\")\n",
    "feature_extractor.save_pretrained(\"./final-fallacy-classifier-whsipersmall\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
